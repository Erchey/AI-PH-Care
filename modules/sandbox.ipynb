{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d15800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vector store...\n",
      "============================================================\n",
      "Setting up medical knowledge base...\n",
      "============================================================\n",
      "\n",
      "ðŸ“ Ingesting folder: ../medical_docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELITEBOOK\\AppData\\Local\\Temp\\ipykernel_14720\\1607499035.py:104: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  self.vector_store.persist()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Ingested 93 chunks from ../medical_docs\\177_1584210847.pdf\n",
      "âœ“ Ingested 280 chunks from ../medical_docs\\BHCPF-2020-Guidelines.pdf\n",
      "âœ“ Ingested 224 chunks from ../medical_docs\\DSD-guidelines-Nigeria.pdf\n",
      "âœ“ Ingested 55 chunks from ../medical_docs\\GUIDELINES-FOR-HOSPITAL-REGISTRATION_compressed.pdf\n",
      "âœ“ Ingested 306 chunks from ../medical_docs\\Guidelines_for_Pain_Mgt_-_Copy_Presented_to_NCH_-20_June_2018_-_FINAL_COPY_-_NEW.pdf\n",
      "âœ“ Ingested 22 chunks from ../medical_docs\\INFECTION-PREVENTION-AND-CONTROL-IPC-STANDARD-OPERATING-PROCEDURE-SOP-.pdf\n",
      "âœ“ Ingested 307 chunks from ../medical_docs\\NGA-National_Guidelines_on_Self-Care_for_Sexual-Reproductive_and_Maternal_Health_2020.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "incorrect startxref pointer(1)\n",
      "parsing for Object Streams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Ingested 452 chunks from ../medical_docs\\nigeria_national_guidelines_2016.pdf\n",
      "âœ“ Ingested 190 chunks from ../medical_docs\\PHCUOR-Implementation-Guidelines.pdf\n",
      "âœ“ Ingested 93 chunks from ../medical_docs\\PPP.pdf\n",
      "âœ“ Ingested 694 chunks from ../medical_docs\\SOP_and_MAP_Post_draft_print_and_indexed_new.pdf\n",
      "âŒ Error ingesting ../medical_docs\\State of health in the African Region.pdf: (ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 8598bd46-a560-484c-b846-ff54ffdeb900)')\n",
      "âœ“ Ingested 535 chunks from ../medical_docs\\Viral haemorrhagic fevers.pdf\n",
      "\n",
      "âœ… Finished ingesting 13 documents (3251 chunks total)\n",
      "âš ï¸  Folder not found (skipping): ./protocols\n",
      "âš ï¸  Folder not found (skipping): ./guidelines\n",
      "\n",
      "ðŸ“„ Ingesting individual sources with custom metadata...\n",
      "  â†’ Fetching: https://www.who.int/publications/guidelines\n",
      "  âŒ Error ingesting https://www.who.int/publications/guidelines: No module named 'bs4'\n",
      "  âš ï¸  File not found (skipping): ./special_docs/emergency_protocols.pdf\n",
      "  âš ï¸  File not found (skipping): ./special_docs/essential_medicines.pdf\n",
      "\n",
      "============================================================\n",
      "âœ… Knowledge base setup complete!\n",
      "   Total chunks ingested: 3251\n",
      "============================================================\n",
      "Answer: The treatment protocol for viral infection depends on the type of virus and the severity of the infection. Here are some general guidelines based on the provided medical knowledge sources:\n",
      "\n",
      "**For general viral infections:**\n",
      "\n",
      "* Ensure optimal oxygenation, conservative fluid therapy, and use of broad-spectrum antibiotics based on local epidemiology (Source 1).\n",
      "* Use antiviral agents, such as Lopinavir/Ritonavir, as recommended by a healthcare professional (Source 1).\n",
      "\n",
      "**For HIV-related viral infections:**\n",
      "\n",
      "* Initiate antiretroviral therapy (ART) as soon as possible following diagnosis, with the goal of achieving sustained viral suppression (Source 3).\n",
      "* For specific opportunistic infections (OIs) related to HIV, such as:\n",
      "\t+ TB: delay ART initiation for 2-4 weeks after starting TB treatment, depending on CD4 count (Source 2).\n",
      "\t+ Cryptococcal meningitis: delay ART initiation for 4-6 weeks after starting treatment (Source 2).\n",
      "\t+ Other OIs like Histoplasmosis, Pneumocystis Pneumonia, and Toxoplasmosis: ART can be initiated immediately (Source 2).\n",
      "\n",
      "**For specific viral infections:**\n",
      "\n",
      "* Cytomegalovirus (CMV) infections: treat with Ganciclovir or Foscarnet (Source 4).\n",
      "* Measles: provide supportive therapy, including anti-pyretics, Vitamin A, and antibiotics as indicated (Source 4).\n",
      "\n",
      "It is essential to note that these guidelines are general and may not be comprehensive or up-to-date. For specific treatment protocols, it is recommended to consult a healthcare professional and refer to the latest medical guidelines and literature.\n",
      "\n",
      "Sources:\n",
      "1. ../medical_docs\\177_1584210847.pdf\n",
      "2. ../medical_docs\\DSD-guidelines-Nigeria.pdf\n",
      "3. ../medical_docs\\nigeria_national_guidelines_2016.pdf\n",
      "4. ../medical_docs\\nigeria_national_guidelines_2016.pdf\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RAG System for Medical Knowledge Retrieval\n",
    "Uses cloud-based Hugging Face embeddings via the HuggingFace Inference API\n",
    "\"\"\"\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceHubEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    WebBaseLoader\n",
    ")\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
    "from langchain.schema import Document\n",
    "from typing import List, Dict\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Load HUGGINGFACEHUB_API_TOKEN from .env\n",
    "\n",
    "\n",
    "class MedicalRAGSystem:\n",
    "    \"\"\"\n",
    "    RAG system for retrieving medical knowledge from documents\n",
    "    Supports PDFs, web links, text files, and markdown\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llm, persist_directory=\"./vector_store2\"):\n",
    "        self.llm = llm\n",
    "        self.persist_directory = persist_directory\n",
    "\n",
    "        # âœ… Initialize cloud-based Hugging Face embeddings\n",
    "        # These embeddings are computed via the Hugging Face Inference API, not locally\n",
    "\n",
    "        model = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "        self.embeddings = HuggingFaceEndpointEmbeddings(\n",
    "            model=model,\n",
    "            task=\"feature-extraction\",\n",
    "            huggingfacehub_api_token=\"\",\n",
    "        )\n",
    "\n",
    "\n",
    "        # Initialize or load vector store\n",
    "        self.vector_store = self._initialize_vector_store()\n",
    "\n",
    "        # Text splitter for chunking documents\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "\n",
    "    def _initialize_vector_store(self):\n",
    "        \"\"\"Initialize or load existing vector store\"\"\"\n",
    "        if os.path.exists(self.persist_directory):\n",
    "            print(\"Loading existing vector store...\")\n",
    "            return Chroma(\n",
    "                persist_directory=self.persist_directory,\n",
    "                embedding_function=self.embeddings\n",
    "            )\n",
    "        else:\n",
    "            print(\"Creating new vector store...\")\n",
    "            return Chroma(\n",
    "                persist_directory=self.persist_directory,\n",
    "                embedding_function=self.embeddings\n",
    "            )\n",
    "\n",
    "    def ingest_document(self, file_path: str, metadata: Dict = None):\n",
    "        \"\"\"\n",
    "        Ingest a document into the RAG system\n",
    "\n",
    "        Args:\n",
    "            file_path: Path to document or URL\n",
    "            metadata: Additional metadata (source, category, etc.)\n",
    "        \"\"\"\n",
    "        if file_path.startswith(\"http://\") or file_path.startswith(\"https://\"):\n",
    "            loader = WebBaseLoader(file_path)\n",
    "            doc_type = \"web\"\n",
    "        elif file_path.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            doc_type = \"pdf\"\n",
    "        elif file_path.endswith(\".md\"):\n",
    "            loader = UnstructuredMarkdownLoader(file_path)\n",
    "            doc_type = \"markdown\"\n",
    "        else:\n",
    "            loader = TextLoader(file_path)\n",
    "            doc_type = \"text\"\n",
    "\n",
    "        documents = loader.load()\n",
    "        splits = self.text_splitter.split_documents(documents)\n",
    "\n",
    "        for doc in splits:\n",
    "            doc.metadata.update({\n",
    "                \"source\": file_path,\n",
    "                \"doc_type\": doc_type,\n",
    "                **(metadata or {})\n",
    "            })\n",
    "\n",
    "        self.vector_store.add_documents(splits)\n",
    "        self.vector_store.persist()\n",
    "\n",
    "        print(f\"âœ“ Ingested {len(splits)} chunks from {file_path}\")\n",
    "        return len(splits)\n",
    "\n",
    "    def ingest_folder(self, folder_path: str = \"../medical_docs\"):\n",
    "        \"\"\"Ingest and vectorize all documents inside a folder.\"\"\"\n",
    "        import glob\n",
    "\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"âš ï¸ Folder not found: {folder_path}\")\n",
    "            return 0\n",
    "\n",
    "        supported_ext = (\".pdf\", \".txt\", \".md\")\n",
    "        files = []\n",
    "        for ext in supported_ext:\n",
    "            files.extend(glob.glob(os.path.join(folder_path, f\"*{ext}\")))\n",
    "\n",
    "        if not files:\n",
    "            print(f\"âš ï¸ No supported documents found in {folder_path}\")\n",
    "            return 0\n",
    "\n",
    "        total_chunks = 0\n",
    "        for file_path in files:\n",
    "            try:\n",
    "                chunks = self.ingest_document(file_path)\n",
    "                total_chunks += chunks\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error ingesting {file_path}: {e}\")\n",
    "\n",
    "        print(f\"\\nâœ… Finished ingesting {len(files)} documents ({total_chunks} chunks total)\")\n",
    "        return total_chunks\n",
    "\n",
    "    def ingest_bulk(self, sources: List[Dict]):\n",
    "        \"\"\"Ingest multiple documents at once\"\"\"\n",
    "        total_chunks = 0\n",
    "        for source in sources:\n",
    "            try:\n",
    "                chunks = self.ingest_document(\n",
    "                    source['path'],\n",
    "                    source.get('metadata', {})\n",
    "                )\n",
    "                total_chunks += chunks\n",
    "            except Exception as e:\n",
    "                print(f\"Error ingesting {source['path']}: {e}\")\n",
    "\n",
    "        print(f\"\\nâœ“ Total: Ingested {total_chunks} chunks from {len(sources)} sources\")\n",
    "        return total_chunks\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 4, filter_metadata: Dict = None):\n",
    "        \"\"\"Retrieve relevant documents for a query\"\"\"\n",
    "        if filter_metadata:\n",
    "            docs = self.vector_store.similarity_search(\n",
    "                query,\n",
    "                k=k,\n",
    "                filter=filter_metadata\n",
    "            )\n",
    "        else:\n",
    "            docs = self.vector_store.similarity_search(query, k=k)\n",
    "\n",
    "        results = []\n",
    "        for doc in docs:\n",
    "            results.append({\n",
    "                \"content\": doc.page_content,\n",
    "                \"metadata\": doc.metadata,\n",
    "                \"source\": doc.metadata.get(\"source\", \"Unknown\")\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    def retrieve_with_scores(self, query: str, k: int = 4):\n",
    "        \"\"\"Retrieve documents with relevance scores\"\"\"\n",
    "        docs_with_scores = self.vector_store.similarity_search_with_score(query, k=k)\n",
    "        results = []\n",
    "        for doc, score in docs_with_scores:\n",
    "            results.append({\n",
    "                \"content\": doc.page_content,\n",
    "                \"metadata\": doc.metadata,\n",
    "                \"relevance_score\": score,\n",
    "                \"source\": doc.metadata.get(\"source\", \"Unknown\")\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    def answer_with_sources(self, query: str, k: int = 4):\n",
    "        \"\"\"Answer a query using RAG with source citations\"\"\"\n",
    "        retrieved_docs = self.retrieve(query, k=k)\n",
    "\n",
    "        if not retrieved_docs:\n",
    "            return {\n",
    "                \"answer\": \"I don't have enough information in my knowledge base to answer this query confidently. Please consult medical guidelines or a senior healthcare professional.\",\n",
    "                \"sources\": []\n",
    "            }\n",
    "\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"Source {i+1}: {doc['content']}\"\n",
    "            for i, doc in enumerate(retrieved_docs)\n",
    "        ])\n",
    "\n",
    "        prompt = f\"\"\"Based on the following medical knowledge sources, answer the healthcare question accurately and concisely.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Medical Knowledge:\n",
    "{context}\n",
    "\n",
    "Instructions:\n",
    "- Provide a clear, evidence-based answer\n",
    "- Cite which sources support your answer (e.g., \"According to Source 1...\")\n",
    "- If information is insufficient, state that clearly\n",
    "- For clinical decisions, recommend consulting a healthcare professional\n",
    "- Use simple language appropriate for PHC workers\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "        response = self.llm.invoke(prompt)\n",
    "\n",
    "        return {\n",
    "            \"answer\": response.content,\n",
    "            \"sources\": retrieved_docs,\n",
    "            \"query\": query\n",
    "        }\n",
    "\n",
    "    def semantic_search(self, query: str, category: str = None, k: int = 5):\n",
    "        \"\"\"Perform semantic search with optional category filtering\"\"\"\n",
    "        filter_dict = {\"category\": category} if category else None\n",
    "        results = self.retrieve(query, k=k, filter_metadata=filter_dict)\n",
    "        return results\n",
    "\n",
    "    def get_emergency_protocol(self, condition: str):\n",
    "        \"\"\"Retrieve emergency protocol using RAG\"\"\"\n",
    "        query = f\"emergency protocol for {condition} immediate treatment steps\"\n",
    "        return self.answer_with_sources(query, k=3)\n",
    "\n",
    "    def get_drug_information(self, drug_name: str):\n",
    "        \"\"\"Retrieve drug information using RAG\"\"\"\n",
    "        query = f\"{drug_name} dosage contraindications side effects interactions\"\n",
    "        return self.answer_with_sources(query, k=3)\n",
    "\n",
    "    def get_diagnostic_guidance(self, symptoms: str):\n",
    "        \"\"\"Get diagnostic guidance for symptoms\"\"\"\n",
    "        query = f\"diagnosis differential diagnosis for patient with {symptoms}\"\n",
    "        return self.answer_with_sources(query, k=4)\n",
    "\n",
    "    def clear_store(self):\n",
    "        \"\"\"Clear the vector store (use with caution)\"\"\"\n",
    "        import shutil\n",
    "        if os.path.exists(self.persist_directory):\n",
    "            shutil.rmtree(self.persist_directory)\n",
    "            print(\"âœ“ Vector store cleared\")\n",
    "\n",
    "        self.vector_store = self._initialize_vector_store()\n",
    "\n",
    "\n",
    "def setup_initial_knowledge_base(rag_system: MedicalRAGSystem):\n",
    "    \"\"\"\n",
    "    Setup initial knowledge base with medical resources\n",
    "    Ingests documents from folders and individual files/URLs\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Setting up medical knowledge base...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Ingest entire folders of documents\n",
    "    folders_to_ingest = [\n",
    "        \"../medical_docs\",\n",
    "        \"./protocols\",\n",
    "        \"./guidelines\",\n",
    "        # Add more folder paths as needed\n",
    "    ]\n",
    "    \n",
    "    total_chunks = 0\n",
    "    \n",
    "    for folder in folders_to_ingest:\n",
    "        if os.path.exists(folder):\n",
    "            print(f\"\\nðŸ“ Ingesting folder: {folder}\")\n",
    "            chunks = rag_system.ingest_folder(folder)\n",
    "            total_chunks += chunks\n",
    "        else:\n",
    "            print(f\"âš ï¸  Folder not found (skipping): {folder}\")\n",
    "    \n",
    "    # 2. Ingest individual files and web sources with specific metadata\n",
    "    individual_sources = [\n",
    "        # Web sources\n",
    "        {\n",
    "            \"path\": \"https://www.who.int/publications/guidelines\",\n",
    "            \"metadata\": {\n",
    "                \"category\": \"guidelines\",\n",
    "                \"source_org\": \"WHO\",\n",
    "                \"language\": \"English\"\n",
    "            }\n",
    "        },\n",
    "        # Specific files with custom metadata\n",
    "        {\n",
    "            \"path\": \"./special_docs/emergency_protocols.pdf\",\n",
    "            \"metadata\": {\n",
    "                \"category\": \"emergency\",\n",
    "                \"doc_type\": \"protocol\",\n",
    "                \"priority\": \"high\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"path\": \"./special_docs/essential_medicines.pdf\",\n",
    "            \"metadata\": {\n",
    "                \"category\": \"drugs\",\n",
    "                \"doc_type\": \"formulary\",\n",
    "                \"priority\": \"high\"\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nðŸ“„ Ingesting individual sources with custom metadata...\")\n",
    "    \n",
    "    for source in individual_sources:\n",
    "        path = source['path']\n",
    "        # Check if it's a URL or if file exists\n",
    "        if path.startswith(\"http://\") or path.startswith(\"https://\"):\n",
    "            try:\n",
    "                print(f\"  â†’ Fetching: {path}\")\n",
    "                chunks = rag_system.ingest_document(path, source.get('metadata'))\n",
    "                total_chunks += chunks\n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ Error ingesting {path}: {e}\")\n",
    "        elif os.path.exists(path):\n",
    "            try:\n",
    "                chunks = rag_system.ingest_document(path, source.get('metadata'))\n",
    "                total_chunks += chunks\n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ Error ingesting {path}: {e}\")\n",
    "        else:\n",
    "            print(f\"  âš ï¸  File not found (skipping): {path}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"âœ… Knowledge base setup complete!\")\n",
    "    print(f\"   Total chunks ingested: {total_chunks}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return total_chunks\n",
    "\n",
    "\n",
    "# Example usage in __main__\n",
    "if __name__ == \"__main__\":\n",
    "    from langchain_groq import ChatGroq\n",
    "    \n",
    "    llm = ChatGroq(\n",
    "        model='meta-llama/llama-4-scout-17b-16e-instruct',\n",
    "        api_key=os.getenv('GROQ_API_KEY'),\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    rag = MedicalRAGSystem(llm)\n",
    "    \n",
    "    # Setup knowledge base (first time only)\n",
    "    # Uncomment the line below to ingest documents\n",
    "    setup_initial_knowledge_base(rag)\n",
    "    \n",
    "    # Or just ingest a single folder\n",
    "    # rag.ingest_folder(\"./medical_docs\")\n",
    "    \n",
    "    # Query the system\n",
    "    result = rag.answer_with_sources(\n",
    "        \"What is the treatment protocol for viral infection?\"\n",
    "    )\n",
    "    \n",
    "    print(\"Answer:\", result['answer'])\n",
    "    print(\"\\nSources:\")\n",
    "    for i, source in enumerate(result['sources'], 1):\n",
    "        print(f\"{i}. {source['source']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9cfeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = HuggingFaceInferenceAPIEmbeddings(\n",
    "            api_key='',  #os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"),\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "chrome= Chroma(\n",
    "    persist_directory='recent_vector_store',\n",
    "    embedding_function=x\n",
    ")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f5801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_document( file_path: str, metadata: Dict = None):\n",
    "    \"\"\"\n",
    "    Ingest a document into the RAG system\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to document or URL\n",
    "        metadata: Additional metadata (source, category, etc.)\n",
    "    \"\"\"\n",
    "    if file_path.startswith(\"http://\") or file_path.startswith(\"https://\"):\n",
    "        loader = WebBaseLoader(file_path)\n",
    "        doc_type = \"web\"\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        doc_type = \"pdf\"\n",
    "    elif file_path.endswith(\".md\"):\n",
    "        loader = UnstructuredMarkdownLoader(file_path)\n",
    "        doc_type = \"markdown\"\n",
    "    else:\n",
    "        loader = TextLoader(file_path)\n",
    "        doc_type = \"text\"\n",
    "\n",
    "    documents = loader.load()\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "\n",
    "    for doc in splits:\n",
    "        doc.metadata.update({\n",
    "            \"source\": file_path,\n",
    "            \"doc_type\": doc_type,\n",
    "            **(metadata or {})\n",
    "        })\n",
    "\n",
    "    chrome.add_documents(splits)\n",
    "    chrome.persist()\n",
    "\n",
    "    print(f\"âœ“ Ingested {len(splits)} chunks from {file_path}\")\n",
    "    return len(splits)\n",
    "\n",
    "\n",
    "def ingest_folder( folder_path: str = \"../medical_docs\"):\n",
    "    \"\"\"Ingest and vectorize all documents inside a folder.\"\"\"\n",
    "    import glob\n",
    "\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"âš ï¸ Folder not found: {folder_path}\")\n",
    "        return 0\n",
    "\n",
    "    supported_ext = (\".pdf\", \".txt\", \".md\")\n",
    "    files = []\n",
    "    for ext in supported_ext:\n",
    "        files.extend(glob.glob(os.path.join(folder_path, f\"*{ext}\")))\n",
    "\n",
    "    if not files:\n",
    "        print(f\"âš ï¸ No supported documents found in {folder_path}\")\n",
    "        return 0\n",
    "\n",
    "    total_chunks = 0\n",
    "    for file_path in files:\n",
    "        try:\n",
    "            chunks = ingest_document(file_path)\n",
    "            total_chunks += chunks\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error ingesting {file_path}: {e}\")\n",
    "\n",
    "    print(f\"\\nâœ… Finished ingesting {len(files)} documents ({total_chunks} chunks total)\")\n",
    "    return total_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e12d7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error ingesting ../medical_docs\\177_1584210847.pdf: Object of type Document is not JSON serializable\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mingest_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 59\u001b[0m, in \u001b[0;36mingest_folder\u001b[1;34m(folder_path)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[43mingest_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m         total_chunks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m chunks\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[27], line 22\u001b[0m, in \u001b[0;36mingest_document\u001b[1;34m(file_path, metadata)\u001b[0m\n\u001b[0;32m     19\u001b[0m     loader \u001b[38;5;241m=\u001b[39m TextLoader(file_path)\n\u001b[0;32m     20\u001b[0m     doc_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 22\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m splits \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(documents)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m splits:\n",
      "File \u001b[1;32mc:\\Users\\ELITEBOOK\\Documents\\Projects\\dca_hackathon\\ai_agent2\\.venv\\lib\\site-packages\\langchain_core\\document_loaders\\base.py:43\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[0;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m        the documents.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ELITEBOOK\\Documents\\Projects\\dca_hackathon\\ai_agent2\\.venv\\lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:305\u001b[0m, in \u001b[0;36mPyPDFLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    304\u001b[0m     blob \u001b[38;5;241m=\u001b[39m Blob\u001b[38;5;241m.\u001b[39mfrom_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n\u001b[1;32m--> 305\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mlazy_parse(blob)\n",
      "File \u001b[1;32mc:\\Users\\ELITEBOOK\\Documents\\Projects\\dca_hackathon\\ai_agent2\\.venv\\lib\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:396\u001b[0m, in \u001b[0;36mPyPDFParser.lazy_parse\u001b[1;34m(self, blob)\u001b[0m\n\u001b[0;32m    394\u001b[0m single_texts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page_number, page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pdf_reader\u001b[38;5;241m.\u001b[39mpages):\n\u001b[1;32m--> 396\u001b[0m     text_from_page \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_text_from_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    397\u001b[0m     images_from_page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_images_from_page(page)\n\u001b[0;32m    398\u001b[0m     all_text \u001b[38;5;241m=\u001b[39m _merge_text_and_extras(\n\u001b[0;32m    399\u001b[0m         [images_from_page], text_from_page\n\u001b[0;32m    400\u001b[0m     )\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32mc:\\Users\\ELITEBOOK\\Documents\\Projects\\dca_hackathon\\ai_agent2\\.venv\\lib\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:378\u001b[0m, in \u001b[0;36mPyPDFParser.lazy_parse.<locals>._extract_text_from_page\u001b[1;34m(page)\u001b[0m\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m page\u001b[38;5;241m.\u001b[39mextract_text()\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m page\u001b[38;5;241m.\u001b[39mextract_text(\n\u001b[0;32m    379\u001b[0m         extraction_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextraction_mode,\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextraction_kwargs,\n\u001b[0;32m    381\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ELITEBOOK\\Documents\\Projects\\dca_hackathon\\ai_agent2\\.venv\\lib\\site-packages\\pypdf\\_page.py:2038\u001b[0m, in \u001b[0;36mPageObject.extract_text\u001b[1;34m(self, orientations, space_width, visitor_operand_before, visitor_operand_after, visitor_text, extraction_mode, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2035\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(orientations, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m   2036\u001b[0m     orientations \u001b[38;5;241m=\u001b[39m (orientations,)\n\u001b[1;32m-> 2038\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2039\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2040\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2041\u001b[0m \u001b[43m    \u001b[49m\u001b[43morientations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2042\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2043\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCONTENTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2044\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisitor_operand_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisitor_operand_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisitor_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2047\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ELITEBOOK\\Documents\\Projects\\dca_hackathon\\ai_agent2\\.venv\\lib\\site-packages\\pypdf\\_page.py:1721\u001b[0m, in \u001b[0;36mPageObject._extract_text\u001b[1;34m(self, obj, pdf, orientations, space_width, content_key, visitor_operand_before, visitor_operand_after, visitor_text)\u001b[0m\n\u001b[0;32m   1714\u001b[0m \u001b[38;5;66;03m# We check all strings are TextStringObjects. ByteStringObjects\u001b[39;00m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;66;03m# are strings where the byte->string encoding was unknown, so adding\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m \u001b[38;5;66;03m# them to the text here would be gibberish.\u001b[39;00m\n\u001b[0;32m   1717\u001b[0m \n\u001b[0;32m   1718\u001b[0m \u001b[38;5;66;03m# Initialize the extractor with the necessary parameters\u001b[39;00m\n\u001b[0;32m   1719\u001b[0m extractor\u001b[38;5;241m.\u001b[39minitialize_extraction(orientations, visitor_text, cmaps)\n\u001b[1;32m-> 1721\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m operands, operator \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperations\u001b[49m:\n\u001b[0;32m   1722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m visitor_operand_before \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1723\u001b[0m         visitor_operand_before(operator, operands, extractor\u001b[38;5;241m.\u001b[39mcm_matrix, extractor\u001b[38;5;241m.\u001b[39mtm_matrix)\n",
      "File \u001b[1;32mc:\\Users\\ELITEBOOK\\Documents\\Projects\\dca_hackathon\\ai_agent2\\.venv\\lib\\site-packages\\pypdf\\generic\\_data_structures.py:1406\u001b[0m, in \u001b[0;36mContentStream.operations\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1403\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m   1404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moperations\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[Any, \u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[0;32m   1405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_operations \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data:\n\u001b[1;32m-> 1406\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_content_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1407\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1408\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_operations\n",
      "File \u001b[1;32mc:\\Users\\ELITEBOOK\\Documents\\Projects\\dca_hackathon\\ai_agent2\\.venv\\lib\\site-packages\\pypdf\\generic\\_data_structures.py:1280\u001b[0m, in \u001b[0;36mContentStream._parse_content_stream\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m   1278\u001b[0m stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peek\u001b[38;5;241m.\u001b[39misalpha() \u001b[38;5;129;01mor\u001b[39;00m peek \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m-> 1280\u001b[0m     operator \u001b[38;5;241m=\u001b[39m \u001b[43mread_until_regex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNameObject\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelimiter_pattern\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m operator \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBI\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1282\u001b[0m         \u001b[38;5;66;03m# begin inline image - a completely different parsing\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m         \u001b[38;5;66;03m# mechanism is required, of course... thanks buddy...\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m operands \u001b[38;5;241m==\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\ELITEBOOK\\Documents\\Projects\\dca_hackathon\\ai_agent2\\.venv\\lib\\site-packages\\pypdf\\_utils.py:264\u001b[0m, in \u001b[0;36mread_until_regex\u001b[1;34m(stream, regex)\u001b[0m\n\u001b[0;32m    262\u001b[0m m \u001b[38;5;241m=\u001b[39m regex\u001b[38;5;241m.\u001b[39msearch(name \u001b[38;5;241m+\u001b[39m tok)\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 264\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m (\u001b[38;5;28mlen\u001b[39m(name) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(tok)), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    265\u001b[0m     name \u001b[38;5;241m=\u001b[39m (name \u001b[38;5;241m+\u001b[39m tok)[: m\u001b[38;5;241m.\u001b[39mstart()]\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ingest_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44857ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12216cb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ELITEBOOK\\Documents\\Projects\\dca_hackathon\\ai_agent2\\.venv\\lib\\site-packages\\requests\\models.py:976\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 976\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    978\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    979\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y \u001b[38;5;241m=\u001b[39m\u001b[43mchrome\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhello\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ELITEBOOK\\Documents\\Projects\\dca_hackathon\\ai_agent2\\.venv\\lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:439\u001b[0m, in \u001b[0;36mChroma.similarity_search_with_score\u001b[1;34m(self, query, k, filter, where_document, **kwargs)\u001b[0m\n\u001b[0;32m    431\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__query_collection(\n\u001b[0;32m    432\u001b[0m         query_texts\u001b[38;5;241m=\u001b[39m[query],\n\u001b[0;32m    433\u001b[0m         n_results\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    437\u001b[0m     )\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__query_collection(\n\u001b[0;32m    441\u001b[0m         query_embeddings\u001b[38;5;241m=\u001b[39m[query_embedding],\n\u001b[0;32m    442\u001b[0m         n_results\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    446\u001b[0m     )\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _results_to_docs_and_scores(results)\n",
      "File \u001b[1;32mc:\\Users\\ELITEBOOK\\Documents\\Projects\\dca_hackathon\\ai_agent2\\.venv\\lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:493\u001b[0m, in \u001b[0;36mHuggingFaceInferenceAPIEmbeddings.embed_query\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    485\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute query embeddings using a HuggingFace transformer model.\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \n\u001b[0;32m    487\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;124;03m        Embeddings for the text.\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ELITEBOOK\\Documents\\Projects\\dca_hackathon\\ai_agent2\\.venv\\lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:482\u001b[0m, in \u001b[0;36mHuggingFaceInferenceAPIEmbeddings.embed_documents\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the embeddings for a list of texts.\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \n\u001b[0;32m    453\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;124;03m        hf_embeddings.embed_documents(texts)\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m    474\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_url,\n\u001b[0;32m    476\u001b[0m     headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_headers,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    480\u001b[0m     },\n\u001b[0;32m    481\u001b[0m )\n\u001b[1;32m--> 482\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ELITEBOOK\\Documents\\Projects\\dca_hackathon\\ai_agent2\\.venv\\lib\\site-packages\\requests\\models.py:980\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    978\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    979\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[1;32m--> 980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "y =chrome.similarity_search_with_score(\n",
    "                json.dumps('hello'),\n",
    "                k=1,\n",
    "                \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d755dc43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': 'Invalid JSON'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "API_URL = \"https://router.huggingface.co/hf-inference/models/Qwen/Qwen3-Embedding-0.6B/pipeline/feature-extraction\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {''}\",\n",
    "}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "query(json.dumps({\n",
    "    \"inputs\": \"Today is a sunny day and I will get some ice cream.\",\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337854fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vector store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELITEBOOK\\AppData\\Local\\Temp\\ipykernel_28132\\3165073762.py:100: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  self.vector_store.persist()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Ingested 93 chunks from ../medical_docs\\177_1584210847.pdf\n",
      "âœ“ Ingested 280 chunks from ../medical_docs\\BHCPF-2020-Guidelines.pdf\n",
      "âœ“ Ingested 224 chunks from ../medical_docs\\DSD-guidelines-Nigeria.pdf\n",
      "âœ“ Ingested 55 chunks from ../medical_docs\\GUIDELINES-FOR-HOSPITAL-REGISTRATION_compressed.pdf\n",
      "âœ“ Ingested 306 chunks from ../medical_docs\\Guidelines_for_Pain_Mgt_-_Copy_Presented_to_NCH_-20_June_2018_-_FINAL_COPY_-_NEW.pdf\n",
      "âœ“ Ingested 22 chunks from ../medical_docs\\INFECTION-PREVENTION-AND-CONTROL-IPC-STANDARD-OPERATING-PROCEDURE-SOP-.pdf\n",
      "âœ“ Ingested 307 chunks from ../medical_docs\\NGA-National_Guidelines_on_Self-Care_for_Sexual-Reproductive_and_Maternal_Health_2020.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "incorrect startxref pointer(1)\n",
      "parsing for Object Streams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Ingested 452 chunks from ../medical_docs\\nigeria_national_guidelines_2016.pdf\n",
      "âœ“ Ingested 190 chunks from ../medical_docs\\PHCUOR-Implementation-Guidelines.pdf\n",
      "âœ“ Ingested 93 chunks from ../medical_docs\\PPP.pdf\n",
      "âœ“ Ingested 694 chunks from ../medical_docs\\SOP_and_MAP_Post_draft_print_and_indexed_new.pdf\n",
      "âœ“ Ingested 1574 chunks from ../medical_docs\\State of health in the African Region.pdf\n",
      "âœ“ Ingested 535 chunks from ../medical_docs\\Viral haemorrhagic fevers.pdf\n",
      "\n",
      "âœ… Finished ingesting 13 documents (4825 chunks total)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4825"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model='meta-llama/llama-4-scout-17b-16e-instruct',\n",
    "    api_key='',\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "rag = MedicalRAGSystem(llm)\n",
    "\n",
    "rag.ingest_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50a9459d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder exists? False\n",
      "c:\\Users\\ELITEBOOK\\Documents\\Projects\\dca_hackathon\\ai_agent2\\modules\n"
     ]
    }
   ],
   "source": [
    "print(\"Folder exists?\", os.path.exists(\"medical_docs\"))\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51138f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ELITEBOOK\\Documents\\Projects\\dca_hackathon\\ai_agent2\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MedicalRAGSystem' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_groq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGroq\n\u001b[0;32m      5\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatGroq(\n\u001b[0;32m      6\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta-llama/llama-4-scout-17b-16e-instruct\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 11\u001b[0m rag \u001b[38;5;241m=\u001b[39m \u001b[43mMedicalRAGSystem\u001b[49m(llm)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Setup knowledge base (first time only)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# setup_initial_knowledge_base(rag)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Query the system\u001b[39;00m\n\u001b[0;32m     17\u001b[0m result \u001b[38;5;241m=\u001b[39m rag\u001b[38;5;241m.\u001b[39manswer_with_sources(\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the treatment protocol for viral infection?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MedicalRAGSystem' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    from langchain_groq import ChatGroq\n",
    "    \n",
    "    llm = ChatGroq(\n",
    "        model='meta-llama/llama-4-scout-17b-16e-instruct',\n",
    "        api_key='',\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    rag = MedicalRAGSystem(llm)\n",
    "    \n",
    "    # Setup knowledge base (first time only)\n",
    "    # setup_initial_knowledge_base(rag)\n",
    "    \n",
    "    # Query the system\n",
    "    result = rag.answer_with_sources(\n",
    "        \"What is the treatment protocol for viral infection?\"\n",
    "    )\n",
    "    \n",
    "    print(\"Answer:\", result['answer'])\n",
    "    print(\"\\nSources:\")\n",
    "    for i, source in enumerate(result['sources'], 1):\n",
    "        print(f\"{i}. {source['source']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6c2978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
